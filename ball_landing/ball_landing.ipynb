{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "#使用PPO算法进行训练自定义游戏环境。游戏环境很简单是二维的，设置为一个小球从天上某个地方自由落体（高度一定，水平位置随机），通过控制左右施加的力让小球落到指定地面范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游戏环境需要实现的方法和返回值\\n__init__(self, render_mode=None)：\\n必须定义action_space和observation_space\\n应该处理render_mode参数\\n\\nreset(self, seed=None, options=None)：\\n必须调用super().reset(seed=seed)\\n必须返回(observation, info)元组\\n\\nstep(self, action)：\\n必须返回(observation, reward, terminated, truncated, info)元组\\n\\nrender(self)：\\n如果支持渲染，必须处理不同的渲染模式\\n对于\"rgb_array\"模式，必须返回RGB数组\\n\\nclose(self)：\\n必须清理所有资源\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"游戏环境需要实现的方法和返回值\n",
    "__init__(self, render_mode=None)：\n",
    "必须定义action_space和observation_space\n",
    "应该处理render_mode参数\n",
    "\n",
    "reset(self, seed=None, options=None)：\n",
    "必须调用super().reset(seed=seed)\n",
    "必须返回(observation, info)元组\n",
    "\n",
    "step(self, action)：\n",
    "必须返回(observation, reward, terminated, truncated, info)元组\n",
    "\n",
    "render(self)：\n",
    "如果支持渲染，必须处理不同的渲染模式\n",
    "对于\"rgb_array\"模式，必须返回RGB数组\n",
    "\n",
    "close(self)：\n",
    "必须清理所有资源\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallLandingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    自定义环境:控制一个从高处落下的球，使其落在指定区域内\n",
    "              @:要求定义的参数内容通过“## 编号.xxxx”标出\n",
    "              @: 参考性的注释通过“#@ 编号.xxx”标出\n",
    "    \n",
    "    状态空间:\n",
    "        - 球的x坐标 (水平位置)\n",
    "        - 球的y坐标 (垂直位置)\n",
    "        - 球的x方向速度\n",
    "        - 球的y方向速度\n",
    "        - 目标区域的中心x坐标\n",
    "    \n",
    "    动作空间:\n",
    "        - 连续值，表示施加在球上的水平力\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 5.metadata是官方建议定义的，包含渲染模式和FPS\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        ## 1.必须调用父类的__init__方法\n",
    "        super(BallLandingEnv, self).__init__()\n",
    "        \n",
    "        # 物理参数\n",
    "        self.gravity = 9.8  # 重力加速度\n",
    "        self.mass = 1.0  # 球的质量\n",
    "        self.time_step = 0.05  # 时间步长\n",
    "        self.max_force = 100.0  # 最大水平力\n",
    "        self.friction = 0.1  # 空气阻力系数\n",
    "        self.vy0=150 #y方向初始速度\n",
    "        \n",
    "        # 环境参数\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 800\n",
    "        self.ball_radius = 15\n",
    "        self.initial_height = 700  # 初始高度\n",
    "        self.target_width = 100  # 目标区域宽度\n",
    "        \n",
    "        ## 2.必须定义动作空间(action_space)\n",
    "        # 动作空间: 连续值，范围为[-1, 1]，表示施加在球上的水平力\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, \n",
    "            high=1.0, \n",
    "            shape=(1,), #@ 1.动作是一个一维向量，包含一个值（标量），表示水平力的大小和方向。例如，action = [0.5] 表示向右施加0.5的力，action = [-0.7] 表示向左施加0.7的力\n",
    "            dtype=np.float32 \n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        观察向量:观察空间定义为 [x, y, vx, vy, target_x]，按照填写顺序，包含 5 个值:\n",
    "        x:小球的水平位置（单位:像素或任意单位）。\n",
    "        y:小球的垂直位置（高度）。\n",
    "        vx:小球的水平速度。\n",
    "        vy:小球的垂直速度。\n",
    "        target_x:目标区域的水平中心位置（例如，目标范围 [-1, 1] 的中心可能是 0）。\n",
    "        \"\"\"\n",
    "        \n",
    "        ## 3.必须定义观察空间(observation_space)\n",
    "        # 观察空间: [x, y, vx, vy, target_x]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -30, -30, 0]), \n",
    "            high=np.array([self.screen_width, self.screen_height, 30, 30, self.screen_width]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        ## 4.渲染相关的官方要求\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # 初始化状态\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        ## 6.必须定义：调用父类reset方法设置随机种子\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # 初始化球的位置和速度\n",
    "        self.x = self.np_random.uniform(self.ball_radius, self.screen_width - self.ball_radius)\n",
    "        self.y = self.ball_radius\n",
    "        self.vx = 0.0\n",
    "        self.vy = self.vy0\n",
    "        \n",
    "        # 设置目标区域\n",
    "        self.target_x = self.np_random.uniform(\n",
    "            self.target_width / 2, \n",
    "            self.screen_width - self.target_width / 2\n",
    "        )\n",
    "        \n",
    "        # 计算观察\n",
    "        self.state = np.array([self.x, self.y, self.vx, self.vy, self.target_x])\n",
    "        \n",
    "        # 设置步数\n",
    "        self.steps = 0\n",
    "        self.max_steps = 500\n",
    "        \n",
    "        ## 7. 渲染初始化\n",
    "        if self.render_mode == \"human\" and self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.display.set_caption(\"Ball Landing Environment\")\n",
    "        if self.render_mode == \"human\" and self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "            \n",
    "        ## 8. 返回初始观察和信息\n",
    "        return self.state, {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        # 获取动作（水平力）\n",
    "        force_x = float(action[0]) * self.max_force\n",
    "        \n",
    "        # 计算加速度\n",
    "        ax = force_x / self.mass - self.friction * self.vx / self.mass\n",
    "        ay = self.gravity #- self.friction * self.vy / self.mass # Cancel g in y axis\n",
    "        \n",
    "        # 更新速度\n",
    "        self.vx += ax * self.time_step\n",
    "        self.vy += ay * self.time_step\n",
    "        \n",
    "        # 更新位置\n",
    "        self.x += self.vx * self.time_step\n",
    "        self.y += self.vy * self.time_step\n",
    "        \n",
    "        # 检查边界碰撞\n",
    "        if self.x < self.ball_radius:\n",
    "            self.x = self.ball_radius\n",
    "            self.vx = -self.vx * 0.8  # 反弹损失一些能量\n",
    "        elif self.x > self.screen_width - self.ball_radius:\n",
    "            self.x = self.screen_width - self.ball_radius\n",
    "            self.vx = -self.vx * 0.8\n",
    "            \n",
    "        # 检查是否到达地面\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if self.y >= self.screen_height - self.ball_radius:\n",
    "            self.y = self.screen_height - self.ball_radius\n",
    "            terminated = True\n",
    "            \n",
    "            # 计算与目标的距离\n",
    "            distance_to_target = abs(self.x - self.target_x)\n",
    "            \n",
    "            # 根据距离给予奖励\n",
    "            if distance_to_target < self.target_width / 2:\n",
    "                # 在目标区域内\n",
    "                normalized_distance = distance_to_target / (self.target_width / 2)\n",
    "                reward = 10.0 * (1.0 - normalized_distance)  # 越靠近中心奖励越高\n",
    "            else:\n",
    "                # 不在目标区域内\n",
    "                reward = -1.0 - min(distance_to_target / 100, 9.0)  # 距离越远惩罚越大，最低-10\n",
    "        \n",
    "        # 更新状态\n",
    "        self.state = np.array([self.x, self.y, self.vx, self.vy, self.target_x])\n",
    "        \n",
    "        # 增加步数\n",
    "        self.steps += 1\n",
    "        \n",
    "        # 检查是否达到最大步数\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        \n",
    "        ## 9. 渲染，如果渲染模式是human，则渲染环境\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "        ## 10.必须计算自己后续的state, reward, terminated, truncated, info并进行返回。info：额外信息字典，这里是{}\n",
    "        return self.state, reward, terminated, truncated, {}\n",
    "    \n",
    "    \"\"\"\n",
    "    渲染模式检查: 检查self.render_mode并相应地处理。\n",
    "\n",
    "    返回值：\n",
    "    如果渲染模式为\"rgb_array\",返回RGB数组。\n",
    "    如果渲染模式为\"human\",更新显示并控制帧率\n",
    "    \"\"\"\n",
    "    ## 11.渲染模式检查\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            return\n",
    "            \n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "            \n",
    "        # 渲染模式为human时\n",
    "        self._render_frame()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        ## 12.初始化pygame（如果尚未初始化）\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            \n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "            \n",
    "        # 填充背景\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        # 绘制目标区域\n",
    "        target_left = self.target_x - self.target_width / 2\n",
    "        pygame.draw.rect(\n",
    "            self.screen, \n",
    "            (0, 255, 0), \n",
    "            pygame.Rect(target_left, self.screen_height - 10, self.target_width, 10)\n",
    "        )\n",
    "        \n",
    "        # 绘制球\n",
    "        pygame.draw.circle(\n",
    "            self.screen,\n",
    "            (255, 0, 0),\n",
    "            (int(self.x), int(self.y)),\n",
    "            self.ball_radius\n",
    "        )\n",
    "        \n",
    "        ## 13.定义：如果是rgb_array模式，返回屏幕的像素数组\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), \n",
    "                axes=(1, 0, 2)\n",
    "            )\n",
    "    \n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"测试脚本\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# 导入环境，在ipynote中不需要\n",
    "#from ball_landing_env import BallLandingEnv\n",
    "\n",
    "# 注册环境\n",
    "register(\n",
    "    id=\"BallLanding-v0\",\n",
    "    entry_point=\"__main__:BallLandingEnv\", #ball_landing_env:BallLandingEnv python文件导入环境，在ipynote中不需要\n",
    ")\n",
    "\n",
    "\n",
    "# 识别模型保存名\n",
    "import os\n",
    "\n",
    "def get_next_model_filename(base_name=\"ball_landing\"):\n",
    "    \"\"\"\n",
    "    检查现有的模型文件并返回下一个可用的序号文件名\n",
    "    例如：如果已存在 ball_landing_0.zip，则返回 ball_landing_1\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        filename = f\"{base_name}_{i}\"\n",
    "        # 检查文件是否存在（注意：SB3保存时会自动添加.zip扩展名）\n",
    "        if not os.path.exists(f\"{filename}.zip\"):\n",
    "            return filename\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 算法参数详解：\n",
    "\n",
    "Python\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    ")\n",
    "参数详解\n",
    "1. \"MlpPolicy\"\n",
    "含义：指定使用多层感知机(Multilayer Perceptron)作为策略网络的架构。\n",
    "说明：这是一个适用于连续动作空间的默认策略网络。MLP策略使用全连接神经网络来处理状态输入并输出动作。\n",
    "何时调整：如果您的环境有特殊的观察空间（如图像），可能需要使用其他策略如\"CnnPolicy\"。\n",
    "1. env\n",
    "含义：训练环境的实例。\n",
    "说明：这是您之前创建的向量化环境 make_vec_env(\"BallLanding-v0\", n_envs=8)。\n",
    "何时调整：通常不需要调整此参数，它就是您的游戏环境。\n",
    "1. verbose=1\n",
    "含义：控制训练过程中输出信息的详细程度。\n",
    "可选值：\n",
    "0：不输出任何信息\n",
    "1：输出训练的基本信息\n",
    "2：输出更详细的训练信息\n",
    "何时调整：如果您想看到更多/更少的训练日志信息。\n",
    "1. learning_rate=0.0003\n",
    "含义：策略网络和价值网络的学习率。\n",
    "说明：控制每次参数更新的步长大小。0.0003是一个适合多数连续控制任务的值。\n",
    "何时调整：\n",
    "如果训练不稳定或收敛太慢，可以尝试较小的值（如0.0001）\n",
    "如果学习速度太慢，可以尝试较大的值（如0.001）\n",
    "1. n_steps=2048\n",
    "含义：每次更新前收集的环境步数。\n",
    "说明：在执行一次策略更新前，每个环境会执行的步骤数。总样本数 = n_steps × n_envs。\n",
    "何时调整：\n",
    "增大这个值可以使训练更稳定，但会减慢训练速度\n",
    "减小这个值可以加快训练，但可能使训练不稳定\n",
    "1. batch_size=64\n",
    "含义：每次梯度更新使用的小批量样本数。\n",
    "说明：从收集的轨迹中随机抽取的样本数量，用于计算每次更新的梯度。\n",
    "何时调整：\n",
    "增大这个值可以使梯度估计更准确，但会增加计算成本\n",
    "通常建议设为n_steps的因子，以确保所有样本都被使用\n",
    "1. n_epochs=10\n",
    "含义：对每批数据执行策略优化的轮数。\n",
    "说明：对同一批数据重复学习的次数。PPO的一个特点是可以多次使用同一批数据进行多轮更新。\n",
    "何时调整：\n",
    "增大可以提高样本效率，但可能导致过拟合\n",
    "减小可以防止过拟合，但可能降低样本效率\n",
    "1. gamma=0.99\n",
    "含义：折扣因子。\n",
    "说明：用于计算未来奖励的折现值，决定了代理对短期和长期奖励的权衡。\n",
    "取值范围：0到1之间，接近1表示更看重长期奖励。\n",
    "何时调整：\n",
    "对于需要长期规划的任务，设置接近1的值（如0.99或0.999）\n",
    "对于只需要短期反馈的任务，可以设置较小的值（如0.9）\n",
    "1. gae_lambda=0.95\n",
    "含义：广义优势估计(GAE)的λ参数。\n",
    "说明：控制偏差和方差之间的权衡，用于计算优势函数。\n",
    "取值范围：0到1之间。\n",
    "何时调整：\n",
    "较高的值（接近1）会导致更高的方差但偏差更小\n",
    "较低的值会导致更低的方差但偏差更大\n",
    "1.  clip_range=0.2\n",
    "含义：PPO算法中的裁剪参数。\n",
    "说明：限制策略更新的幅度，防止过大的策略变化导致训练不稳定。\n",
    "何时调整：\n",
    "减小这个值可以使训练更稳定，但学习速度会变慢\n",
    "增大这个值可以加快学习，但可能导致训练不稳定\n",
    "1.  tensorboard_log=\"./tensorboard_logs/\"\n",
    "含义：TensorBoard日志的保存路径。\n",
    "说明：指定训练过程中的指标（如奖励、损失等）的保存位置，可以使用TensorBoard可视化这些指标。\n",
    "何时调整：如果您希望将日志保存在不同的位置，或者不想使用TensorBoard（设为None）。\n",
    "如何根据您的环境调整这些参数\n",
    "对于初学者：建议先使用默认参数，观察训练效果。\n",
    "\n",
    "学习率调整：\n",
    "\n",
    "如果训练不稳定（奖励波动大），尝试减小学习率\n",
    "如果学习太慢，尝试增加学习率\n",
    "样本效率调整：\n",
    "\n",
    "增加 n_epochs 和 batch_size 可以提高样本效率\n",
    "但要注意可能导致过拟合和训练不稳定\n",
    "长期vs短期奖励：\n",
    "\n",
    "如果您的任务需要长期规划，保持 gamma 接近1\n",
    "例如在您的球落地环境中，小球需要规划整个下落轨迹，所以较高的gamma值（0.99）是合适的\n",
    "探索与利用平衡：\n",
    "\n",
    "PPO算法会自动调整探索程度，但您可以通过调整 clip_range 间接影响探索行为\n",
    "较大的 clip_range 允许更剧烈的策略变化，可能导致更多探索"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO 参数选择\n",
    "1. n_steps参数建议\n",
    "建议值范围：回合长度的2-4倍\n",
    "理由： \n",
    "理想情况下，n_steps 应该是回合长度的2-4倍\n",
    "对于120步/回合的环境，建议值为240-480\n",
    "这样每个样本收集周期会包含2-4个完整回合\n",
    "既能学习足够的时序依赖关系，又能保持较高的更新频率\n",
    "具体推荐：\n",
    "如果重视样本效率：n_steps=240（2倍回合长度）\n",
    "如果重视训练稳定性：n_steps=360（3倍回合长度）\n",
    "如果重视长期依赖学习：n_steps=480（4倍回合长度）\n",
    "\n",
    "1. batch_size参数建议\n",
    "建议值范围：是 n_steps 的1/4到1/8\n",
    "理由：\n",
    "理想情况下，batch_size 应该是 n_steps 的1/4到1/8\n",
    "对于建议的 n_steps 范围，对应的 batch_size 应为60-120\n",
    "GPU训练时可以使用较大的batch_size以提高并行效率\n",
    "具体推荐：\n",
    "CPU训练时：batch_size=64\n",
    "GPU训练时：batch_size=128\n",
    "内存受限时：batch_size=32（保持您当前的设置）\n",
    "\n",
    "3. n_epochs参数建议\n",
    "与 n_steps 和 batch_size 密切相关\n",
    "建议值范围：4-10\n",
    "理由：\n",
    "较小的 n_steps 通常需要较多的 n_epochs 来充分利用数据\n",
    "对于建议的 n_steps 和 batch_size，建议 n_epochs 为：\n",
    "当 n_steps=240 时：n_epochs=8-10\n",
    "当 n_steps=360 时：n_epochs=6-8\n",
    "当 n_steps=480 时：n_epochs=4-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as th\n",
    "from stable_baselines3.common.noise import ActionNoise, NormalActionNoise\n",
    "\n",
    "# TD3的衰减噪声，常数噪声效果不行\n",
    "class DecayingActionNoise(ActionNoise):\n",
    "    \"\"\"\n",
    "    衰减高斯噪声，可用于TD3的探索\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_std, final_std, decay_rate, action_dim):\n",
    "        super().__init__()  # 调用父类构造函数\n",
    "        self.initial_std = initial_std\n",
    "        self.final_std = final_std\n",
    "        self.decay_rate = decay_rate\n",
    "        self.action_dim = action_dim\n",
    "        self.current_std = initial_std\n",
    "        \n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"生成噪声样本\"\"\"\n",
    "        noise = np.random.normal(0, self.current_std, self.action_dim)\n",
    "        self.current_std = max(self.final_std, \n",
    "                            self.current_std * self.decay_rate)\n",
    "        return noise\n",
    "        \n",
    "    def reset(self) -> None:\n",
    "        \"\"\"重置噪声状态\"\"\"\n",
    "        self.current_std = self.initial_std\n",
    "\n",
    "def main():\n",
    "    # 创建向量化环境以提高训练效率\n",
    "    env = make_vec_env(\"BallLanding-v0\", n_envs=8) # python文件使用，ipynb不需要\n",
    "    #env = gym.make(\"BallLanding-v0\", render_mode=None)\n",
    "\n",
    "    # model = SAC(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     env,\n",
    "    #     verbose=1,\n",
    "    #     learning_rate=0.0003,\n",
    "    #     buffer_size=100000,\n",
    "    #     batch_size=256,\n",
    "    #     tau=0.005,\n",
    "    #     gamma=0.99,\n",
    "    #     train_freq=1,\n",
    "    #     gradient_steps=1,\n",
    "    #     ent_coef=\"auto\",\n",
    "    #     tensorboard_log=\"./tensorboard_logs/\"\n",
    "    # )\n",
    "\n",
    "    # # 设置噪声\n",
    "    # n_actions = env.action_space.shape[0]\n",
    "    # # 创建衰减噪声\n",
    "    # action_noise = DecayingActionNoise(\n",
    "    #     initial_std=0.05,    # 初始噪声大小\n",
    "    #     final_std=0.001,     # 最终噪声大小\n",
    "    #     decay_rate=0.9998, # 衰减率\n",
    "    #     action_dim=n_actions\n",
    "    # )\n",
    "    # # 更深层的网络架构\n",
    "    # policy_kwargs = dict(\n",
    "    #     net_arch=dict(\n",
    "    #         pi=[256, 256],     # 策略网络较深\n",
    "    #         qf=[256, 256, 128, 64] # Q网络更深\n",
    "    #     ),\n",
    "    #     activation_fn=th.nn.ReLU,\n",
    "    #     # 尝试不同初始化方法\n",
    "    #     # initializer=lambda x: th.nn.init.orthogonal_(x, gain=1.0)\n",
    "    # )\n",
    "    # model = TD3(\n",
    "    #     \"MlpPolicy\",\n",
    "    #     env,\n",
    "    #     seed=3,\n",
    "    #     verbose=1,\n",
    "    #     learning_rate=0.0001,\n",
    "    #     buffer_size=100000,\n",
    "    #     batch_size=256,\n",
    "    #     tau=0.005,\n",
    "    #     gamma=0.99,\n",
    "    #     policy_delay=4,\n",
    "    #     target_policy_noise=0.2,\n",
    "    #     target_noise_clip=0.4,\n",
    "    #     tensorboard_log=\"./tensorboard_logs/\"\n",
    "    # )\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=0.0001, # 过大学习率(0.001)会导致学习率后期下降\n",
    "        n_steps=1024,         # 更广的steps(2048,4096)可以提供更深入的学习，学习速度变慢但后期效果会好\n",
    "        batch_size=16,        # batch_size过高会学到很多没用的,训练效果差\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        tensorboard_log=\"./tensorboard_logs/\"\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    total_timesteps = 1_000_000\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "    \n",
    "    # 按序号保存模型\n",
    "    model_filename = get_next_model_filename()\n",
    "    model.save(model_filename)\n",
    "    print(f\"模型已保存为: {model_filename}.zip\")\n",
    "    \n",
    "    # 评估模型\n",
    "    eval_env = gym.make(\"BallLanding-v0\", render_mode=\"human\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # 展示训练后的模型\n",
    "    obs, _ = eval_env.reset()\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            obs, _ = eval_env.reset()\n",
    "            \n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./tensorboard_logs/PPO_32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">type: float64</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual \u001b[0m\n",
       "\u001b[33mtype: float64\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method is not within the observation space.</span>\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90       |\n",
      "|    ep_rew_mean     | -1.88    |\n",
      "| time/              |          |\n",
      "|    fps             | 15265    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | -1.25        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1126         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025631164 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0256       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.173        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.615        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | -0.61        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042980914 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.482        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 0.505        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 0.594       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 785         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005353603 |\n",
      "|    clip_fraction        | 0.046       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.434       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 1.57         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 727          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058724643 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.164        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0053      |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 0.573        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 2.56         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 693          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073177433 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.748        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.519        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.94         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 2.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 671         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 85          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005296807 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.229       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    std                  | 0.966       |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 3.11         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055793617 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.54         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.218        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 1.29         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 2.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196048 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.3         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 1.09        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 3.48         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062985215 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.731        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    std                  | 0.921        |\n",
      "|    value_loss           | 0.797        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 3.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008714013 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.632       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    std                  | 0.907       |\n",
      "|    value_loss           | 0.813       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 4.13         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069140093 |\n",
      "|    clip_fraction        | 0.0768       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.191        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 0.897        |\n",
      "|    value_loss           | 0.982        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 4.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008420793 |\n",
      "|    clip_fraction        | 0.0918      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00209    |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 0.639       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 5.59         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 186          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103578465 |\n",
      "|    clip_fraction        | 0.0877       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.762        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.234        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    std                  | 0.889        |\n",
      "|    value_loss           | 0.774        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90         |\n",
      "|    ep_rew_mean          | 6.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 611        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00926823 |\n",
      "|    clip_fraction        | 0.0903     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.169      |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.00163   |\n",
      "|    std                  | 0.892      |\n",
      "|    value_loss           | 0.691      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 6.1          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 609          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 215          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073363646 |\n",
      "|    clip_fraction        | 0.0813       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.422        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000601    |\n",
      "|    std                  | 0.885        |\n",
      "|    value_loss           | 0.479        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 6.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010547621 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00337    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 0.439       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90         |\n",
      "|    ep_rew_mean          | 7.03       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 603        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 244        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01006965 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 0.814      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.173      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.00231   |\n",
      "|    std                  | 0.85       |\n",
      "|    value_loss           | 0.461      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 258         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009737271 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    std                  | 0.834       |\n",
      "|    value_loss           | 0.405       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011499258 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.000753    |\n",
      "|    std                  | 0.825       |\n",
      "|    value_loss           | 0.316       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 599         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010990914 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 0.811       |\n",
      "|    value_loss           | 0.261       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009147237 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.148       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.000755    |\n",
      "|    std                  | 0.816       |\n",
      "|    value_loss           | 0.239       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 315         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012572583 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    std                  | 0.825       |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.04        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 330         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009560078 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 0.219       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 344         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012430568 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0825      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    std                  | 0.841       |\n",
      "|    value_loss           | 0.352       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 359         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010922463 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.00219     |\n",
      "|    std                  | 0.831       |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 7.94         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 591          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 373          |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109830685 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0111       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.00308      |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 0.288        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010610051 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.171       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 6.86e-05    |\n",
      "|    std                  | 0.812       |\n",
      "|    value_loss           | 0.213       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012016627 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0649      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 7.93        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010217928 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    std                  | 0.795       |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010596695 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.00206     |\n",
      "|    std                  | 0.792       |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011121871 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00157     |\n",
      "|    std                  | 0.777       |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011095084 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00609     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.000201   |\n",
      "|    std                  | 0.759       |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012455081 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.0035      |\n",
      "|    std                  | 0.763       |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 587         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 487         |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012165021 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000151   |\n",
      "|    std                  | 0.746       |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90         |\n",
      "|    ep_rew_mean          | 8.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 587        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 501        |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01081421 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0811     |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | 0.00204    |\n",
      "|    std                  | 0.738      |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010635253 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.0038      |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.81        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013561037 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 541         |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013520706 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0396      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.004       |\n",
      "|    std                  | 0.721       |\n",
      "|    value_loss           | 0.0839      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 554         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012878348 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00482    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00405     |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 0.0753      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 567         |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011581922 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00541     |\n",
      "|    std                  | 0.709       |\n",
      "|    value_loss           | 0.0872      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 90           |\n",
      "|    ep_rew_mean          | 8.87         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 592          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 580          |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145325195 |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00268      |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | 0.000342     |\n",
      "|    std                  | 0.703        |\n",
      "|    value_loss           | 0.0794       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 593         |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013075435 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0633      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 0.059       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 606         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014068735 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    std                  | 0.699       |\n",
      "|    value_loss           | 0.068       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 620         |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015485184 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 633         |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019724686 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00623    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00477     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 646         |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015708089 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0139      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.004       |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 0.0742      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.97        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015989855 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.00227     |\n",
      "|    std                  | 0.665       |\n",
      "|    value_loss           | 0.0572      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 673         |\n",
      "|    total_timesteps      | 401408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011690043 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0655      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.00266     |\n",
      "|    std                  | 0.663       |\n",
      "|    value_loss           | 0.0599      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.96        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 687         |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015650924 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00878     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.00317     |\n",
      "|    std                  | 0.662       |\n",
      "|    value_loss           | 0.0578      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 9.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 595         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014223397 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00961     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | 0.00336     |\n",
      "|    std                  | 0.655       |\n",
      "|    value_loss           | 0.0656      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 716         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015786322 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00533     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00358     |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 0.0542      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 730         |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015716746 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.005      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 0.0592      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90          |\n",
      "|    ep_rew_mean          | 8.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014132719 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 0.0773      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, TD3, SAC, A2C, DDPG\n",
    "import time\n",
    "import os\n",
    "\n",
    "def test(model_path, algorithm, episodes=5, render=True):\n",
    "    \"\"\"\n",
    "    测试指定的强化学习模型\n",
    "    \n",
    "    参数:\n",
    "        model_path: 模型文件的完整路径\n",
    "        algorithm: 算法类型 (PPO, TD3, SAC, A2C, DDPG)\n",
    "        episodes: 测试回合数\n",
    "        render: 是否渲染环境\n",
    "    \"\"\"\n",
    "    # 支持的算法\n",
    "    ALGORITHMS = {\n",
    "        \"ppo\": PPO,\n",
    "        \"td3\": TD3,\n",
    "        \"sac\": SAC,\n",
    "        \"a2c\": A2C,\n",
    "        \"ddpg\": DDPG\n",
    "    }\n",
    "    \n",
    "    # 检查算法类型是否有效\n",
    "    alg_lower = algorithm.lower()\n",
    "    if alg_lower not in ALGORITHMS:\n",
    "        print(f\"不支持的算法类型: {algorithm}. 可用的算法: {list(ALGORITHMS.keys())}\")\n",
    "        return\n",
    "    \n",
    "    # 检查模型文件是否存在\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"模型文件不存在: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    # 加载模型\n",
    "    try:\n",
    "        AlgorithmClass = ALGORITHMS[alg_lower]\n",
    "        model = AlgorithmClass.load(model_path, custom_objects={\n",
    "            'use_sde': False,\n",
    "            'sde_sample_freq': -1\n",
    "        })\n",
    "        print(f\"成功加载模型: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"加载模型失败: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 创建环境\n",
    "    render_mode = \"human\" if render else None\n",
    "    env = gym.make(\"BallLanding-v0\", render_mode=render_mode)\n",
    "    \n",
    "    # 运行测试回合\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # 根据当前观察预测动作\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # 检查是否完成\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # 添加一点延迟，使观察更容易\n",
    "            if render:\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: 步数 = {step_count}, 奖励 = {episode_reward:.2f}\")\n",
    "    \n",
    "    # 计算统计信息\n",
    "    mean_reward = total_reward / episodes\n",
    "    print(f\"\\n测试结果:\")\n",
    "    print(f\"算法: {alg_lower.upper()}\")\n",
    "    print(f\"模型: {model_path}\")\n",
    "    print(f\"回合数: {episodes}\")\n",
    "    print(f\"平均奖励: {mean_reward:.2f}\")\n",
    "    print(f\"最高奖励: {max(rewards):.2f}\")\n",
    "    print(f\"最低奖励: {min(rewards):.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return model  # 返回加载的模型，以便进一步使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载模型: ppo_ball_landing_7.zip\n",
      "Episode 1: 步数 = 90, 奖励 = 0.56\n",
      "Episode 2: 步数 = 90, 奖励 = 6.21\n",
      "Episode 3: 步数 = 90, 奖励 = 9.76\n",
      "Episode 4: 步数 = 90, 奖励 = -1.65\n",
      "Episode 5: 步数 = 90, 奖励 = 5.30\n",
      "\n",
      "测试结果:\n",
      "算法: PPO\n",
      "模型: ppo_ball_landing_7.zip\n",
      "回合数: 5\n",
      "平均奖励: 4.04\n",
      "最高奖励: 9.76\n",
      "最低奖励: -1.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f483ec4c3d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动指定模型文件路径和算法类型\n",
    "model_file = \"ppo_ball_landing_7.zip\"  # 替换为你的模型文件路径\n",
    "algorithm_type = \"PPO\"  # 替换为对应的算法类型\n",
    "test(model_file, algorithm_type, episodes=5, render=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阅读TensorBoard\n",
    "## rollout数据\n",
    "1. rollout/ep_len_mean - 平均回合长度\n",
    "定义：此指标表示每个回合（episode）的平均步数。\n",
    "技术解释：\n",
    "在强化学习中，一个\"回合\"是指从环境重置到终止状态的完整序列\n",
    "这个指标计算了最近N个回合的平均步数\n",
    "在 Stable Baselines3 中，默认是对最近100个回合取平均\n",
    "\n",
    "如何解读曲线：\n",
    "\n",
    "上升趋势：表明代理能够在环境中存活更长时间，通常是积极的信号\n",
    "在\"保持平衡\"类任务中：更长的回合长度直接意味着更好的性能\n",
    "在有固定终止条件的任务中：可能表示代理学会了避免失败条件\n",
    "下降趋势：解读取决于环境类型\n",
    "在\"尽快到达目标\"类任务中：较短的回合长度可能表示代理学会了更有效的策略\n",
    "在大多数其他环境中：可能表示代理性能下降或正在探索新策略\n",
    "稳定值：表示代理的行为模式已经稳定，可能达到了该策略下的最佳表现\n",
    "\n",
    "1. rollout/ep_rew_mean - 平均回合奖励\n",
    "定义：此指标表示每个回合获得的平均总奖励。\n",
    "技术解释：\n",
    "计算最近N个回合（通常是100个）中每个回合获得的总奖励的平均值\n",
    "这是评估代理性能的最直接且最重要的指标\n",
    "在 Stable Baselines3 中实时更新，显示训练过程中的性能变化\n",
    "如何解读曲线：\n",
    "\n",
    "上升趋势：表示代理性能正在改善，这几乎总是积极的信号\n",
    "陡峭上升：快速学习，可能发现了新的有效策略\n",
    "缓慢上升：渐进改善，通常是稳健学习的标志\n",
    "平稳趋势：可能表示学习已经达到饱和或局部最优\n",
    "\n",
    "如果值较高：可能已接近环境的最优性能\n",
    "如果值较低：可能陷入局部最优，需要调整超参数或探索策略\n",
    "下降趋势：通常表示问题\n",
    "\n",
    "暂时下降：可能是探索新策略的过程\n",
    "持续下降：可能是学习率过高或其他超参数问题\n",
    "\n",
    "对于球落地环境：\n",
    "理想的曲线应该从负值或低值开始（初始随机策略）\n",
    "随着训练进行，应该持续上升\n",
    "最终应该达到并稳定在一个正的高值，表示代理已学会将球准确落在目标区域\n",
    "\n",
    "## train数据\n",
    "1. approx_kl - 近似 KL 散度\n",
    "含义：测量更新前后策略分布的差异程度。\n",
    "理想曲线：\n",
    "通常应该在 0.01 到 0.05 之间\n",
    "应相对稳定，略有波动但不应该持续增长\n",
    "偶尔的峰值是正常的，但不应经常出现\n",
    "解读：\n",
    "太低（接近0）：策略几乎没有更新，学习停滞\n",
    "太高（>0.1）：策略变化太剧烈，可能导致训练不稳定\n",
    "稳定在适当范围：表明策略更新适度，学习正常进行\n",
    "\n",
    "2. clip_fraction - 裁剪比例\n",
    "含义：被 PPO 的裁剪机制裁剪的样本比例。\n",
    "理想曲线：\n",
    "训练初期可能较高（0.1-0.3）\n",
    "随着训练进行应逐渐降低并稳定在较低水平（<0.1）\n",
    "解读：\n",
    "持续较高（>0.2）：表明clip_range可能设置得太小，限制了学习\n",
    "几乎为零：可能clip_range太大或学习率太小\n",
    "从高到低再稳定：理想的模式，表明策略逐渐收敛\n",
    "\n",
    "3. clip_range - 裁剪范围\n",
    "含义：PPO 算法中限制策略更新幅度的参数值。\n",
    "理想曲线：\n",
    "如果使用固定值（如0.2），应该是一条水平线\n",
    "如果使用衰减策略，应该是一条平滑下降的曲线\n",
    "解读：\n",
    "这通常是一个设置值而非监控指标\n",
    "确认它是否符合您的预期设置\n",
    "\n",
    "4. entropy_loss - 熵损失\n",
    "含义：衡量策略的不确定性/随机性。\n",
    "理想曲线：\n",
    "训练初期较高，随后逐渐下降\n",
    "不应该太快降到接近零\n",
    "解读：\n",
    "持续较高：策略保持高随机性，可能在过度探索\n",
    "快速降至接近零：策略变得过于确定，可能陷入局部最优\n",
    "缓慢下降：良好的探索-利用权衡，策略逐渐从探索转向利用\n",
    "\n",
    "5. explained_variance - 解释方差\n",
    "含义：价值函数预测与实际回报的匹配程度。\n",
    "理想曲线：\n",
    "从负值或接近0开始，逐渐上升并稳定在接近1的位置\n",
    "解读：\n",
    "接近1：价值函数很好地预测了回报，学习有效\n",
    "接近0：价值函数仅预测平均回报，没有提供额外信息\n",
    "负值：价值函数预测比使用平均值更差，表明严重问题\n",
    "\n",
    "6. loss - 总损失\n",
    "含义：PPO 的总体损失函数。\n",
    "理想曲线：\n",
    "训练初期较高，随后应该逐渐下降并趋于稳定\n",
    "可能有波动，但总体趋势应下降\n",
    "解读：\n",
    "持续下降：学习正常进行\n",
    "停滞不降：可能遇到学习瓶颈\n",
    "剧烈波动：训练不稳定，可能需要调整学习率或批量大小\n",
    "\n",
    "7. policy_gradient_loss - 策略梯度损失\n",
    "含义：代表策略网络更新的主要损失组件。\n",
    "理想曲线：\n",
    "应该是负值（因为 PPO 最大化此值）\n",
    "训练初期可能波动较大，随后应该变得更稳定\n",
    "不应有持续上升趋势（变得更正）\n",
    "解读：\n",
    "持续变得更负：策略正在改进\n",
    "趋于零：策略更新变小，可能接近收敛或学习停滞\n",
    "变为正值或波动剧烈：训练不稳定，需要调整\n",
    "\n",
    "8. std - 动作标准差\n",
    "含义：策略输出的动作分布的标准差。\n",
    "理想曲线：\n",
    "训练初期较高（更多探索）\n",
    "随着训练进行应逐渐减小并稳定（更多利用）\n",
    "不应该太快降到接近零\n",
    "解读：\n",
    "保持较高：策略保持高随机性，持续探索\n",
    "迅速降至接近零：策略变得确定性太快，可能过早收敛\n",
    "缓慢下降并保持合理水平：良好的探索-利用平衡\n",
    "\n",
    "9. value_loss - 价值损失\n",
    "含义：价值函数预测误差的度量。\n",
    "理想曲线：\n",
    "训练初期较高，随后应逐渐下降并稳定\n",
    "可能永远不会降到非常接近零\n",
    "解读：\n",
    "持续下降：价值估计在改进\n",
    "下降后稳定在低水平：价值函数学习良好\n",
    "波动剧烈或上升：价值估计不稳定或环境本身有高方差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型文件: ball_landing_2.zip\n",
      "训练参数:\n",
      "{\n",
      "  \"policy_class\": \"<class 'stable_baselines3.common.policies.ActorCriticPolicy'>\",\n",
      "  \"verbose\": 1,\n",
      "  \"policy_kwargs\": {},\n",
      "  \"num_timesteps\": 1000320,\n",
      "  \"_total_timesteps\": 1000000,\n",
      "  \"_num_timesteps_at_start\": 0,\n",
      "  \"seed\": null,\n",
      "  \"action_noise\": null,\n",
      "  \"start_time\": 1756365968595611046,\n",
      "  \"learning_rate\": 0.001,\n",
      "  \"tensorboard_log\": \"./tensorboard_logs/\",\n",
      "  \"_last_episode_starts\": \"[ True  True  True  True  True  True  True  True]\",\n",
      "  \"_last_original_obs\": null,\n",
      "  \"_episode_num\": 0,\n",
      "  \"use_sde\": false,\n",
      "  \"sde_sample_freq\": -1,\n",
      "  \"_current_progress_remaining\": -0.000320000000000098,\n",
      "  \"_stats_window_size\": 100,\n",
      "  \"ep_info_buffer\": \"deque([{'r': -4.343323, 'l': 120, 't': 461.576049}, {'r': -1.874965, 'l': 120, 't': 461.575033}, {'r': 5.404538, 'l': 120, 't': 461.573412}, {'r': -5.894273, 'l': 120, 't': 461.572732}, {'r': 7.198115, 'l': 120, 't': 462.400667}, {'r': -5.268167, 'l': 120, 't': 462.399589}, {'r': -4.7584, 'l': 120, 't': 462.398565}, {'r': -4.250543, 'l': 120, 't': 462.397584}, {'r': -4.691839, 'l': 120, 't': 462.396621}, {'r': 6.160277, 'l': 120, 't': 462.395621}, {'r': -2.696908, 'l': 120, 't': 462.39401}, {'r': -1.667908, 'l': 120, 't': 462.393337}, {'r': -3.610118, 'l': 120, 't': 462.458486}, {'r': -1.82423, 'l': 120, 't': 462.457366}, {'r': -4.526333, 'l': 120, 't': 462.45634}, {'r': -2.567339, 'l': 120, 't': 462.455342}, {'r': -3.358255, 'l': 120, 't': 462.454351}, {'r': -3.852579, 'l': 120, 't': 462.453335}, {'r': -2.307288, 'l': 120, 't': 462.451713}, {'r': 4.817666, 'l': 120, 't': 462.451033}, {'r': 6.296363, 'l': 120, 't': 463.285308}, {'r': -2.722749, 'l': 120, 't': 463.284199}, {'r': 0.892375, 'l': 120, 't': 463.283176}, {'r': -4.532321, 'l': 120, 't': 463.282181}, {'r': 9.964669, 'l': 120, 't': 463.281193}, {'r': 0.530712, 'l': 120, 't': 463.280181}, {'r': -2.142695, 'l': 120, 't': 463.278563}, {'r': -3.866384, 'l': 120, 't': 463.277889}, {'r': -4.220055, 'l': 120, 't': 463.344824}, {'r': -5.052394, 'l': 120, 't': 463.343706}, {'r': -1.641993, 'l': 120, 't': 463.342682}, {'r': -1.546509, 'l': 120, 't': 463.341685}, {'r': -3.502957, 'l': 120, 't': 463.340697}, {'r': -1.575301, 'l': 120, 't': 463.339683}, {'r': 5.175585, 'l': 120, 't': 463.338064}, {'r': -4.023615, 'l': 120, 't': 463.337387}, {'r': -2.111207, 'l': 120, 't': 464.17469}, {'r': -2.904362, 'l': 120, 't': 464.173568}, {'r': -1.952889, 'l': 120, 't': 464.172542}, {'r': 8.269684, 'l': 120, 't': 464.171543}, {'r': -5.169674, 'l': 120, 't': 464.170552}, {'r': -4.460886, 'l': 120, 't': 464.169536}, {'r': -4.842642, 'l': 120, 't': 464.167912}, {'r': -5.712819, 'l': 120, 't': 464.167232}, {'r': -1.524659, 'l': 120, 't': 464.234215}, {'r': -5.425742, 'l': 120, 't': 464.233119}, {'r': -5.155311, 'l': 120, 't': 464.232098}, {'r': 3.466959, 'l': 120, 't': 464.231101}, {'r': -1.931295, 'l': 120, 't': 464.230111}, {'r': -4.715448, 'l': 120, 't': 464.229096}, {'r': -2.937693, 'l': 120, 't': 464.227475}, {'r': -1.502373, 'l': 120, 't': 464.226797}, {'r': -3.211018, 'l': 120, 't': 465.052476}, {'r': -1.659893, 'l': 120, 't': 465.051356}, {'r': -5.330913, 'l': 120, 't': 465.050329}, {'r': -3.730473, 'l': 120, 't': 465.049334}, {'r': -3.87746, 'l': 120, 't': 465.048342}, {'r': -4.574912, 'l': 120, 't': 465.047326}, {'r': -3.981844, 'l': 120, 't': 465.045703}, {'r': -3.905173, 'l': 120, 't': 465.045024}, {'r': -3.540702, 'l': 120, 't': 465.108022}, {'r': -2.759633, 'l': 120, 't': 465.106901}, {'r': 4.368092, 'l': 120, 't': 465.105877}, {'r': -4.217281, 'l': 120, 't': 465.10488}, {'r': -3.605138, 'l': 120, 't': 465.10389}, {'r': -2.051364, 'l': 120, 't': 465.102872}, {'r': -5.054029, 'l': 120, 't': 465.101249}, {'r': -5.602322, 'l': 120, 't': 465.100571}, {'r': -4.669629, 'l': 120, 't': 465.913658}, {'r': -2.238464, 'l': 120, 't': 465.912561}, {'r': -2.056967, 'l': 120, 't': 465.911538}, {'r': -4.376568, 'l': 120, 't': 465.910539}, {'r': -1.72315, 'l': 120, 't': 465.909549}, {'r': -5.136658, 'l': 120, 't': 465.908533}, {'r': -2.451367, 'l': 120, 't': 465.90691}, {'r': 8.504622, 'l': 120, 't': 465.906232}, {'r': -3.065526, 'l': 120, 't': 465.97203}, {'r': -5.810442, 'l': 120, 't': 465.970909}, {'r': -4.546169, 'l': 120, 't': 465.969884}, {'r': -2.999163, 'l': 120, 't': 465.968886}, {'r': -3.377145, 'l': 120, 't': 465.9679}, {'r': -1.509707, 'l': 120, 't': 465.966886}, {'r': -2.848273, 'l': 120, 't': 465.965266}, {'r': -3.655987, 'l': 120, 't': 465.964588}, {'r': -5.905836, 'l': 120, 't': 466.791703}, {'r': -1.508219, 'l': 120, 't': 466.790581}, {'r': -2.105525, 'l': 120, 't': 466.789553}, {'r': -2.443454, 'l': 120, 't': 466.788553}, {'r': -3.576051, 'l': 120, 't': 466.787562}, {'r': -2.140063, 'l': 120, 't': 466.786546}, {'r': -3.458939, 'l': 120, 't': 466.784923}, {'r': -3.840803, 'l': 120, 't': 466.784243}, {'r': -2.65469, 'l': 120, 't': 466.850134}, {'r': 4.411817, 'l': 120, 't': 466.849123}, {'r': -5.381575, 'l': 120, 't': 466.848124}, {'r': -3.256056, 'l': 120, 't': 466.847153}, {'r': -3.614104, 'l': 120, 't': 466.846189}, {'r': -2.562118, 'l': 120, 't': 466.845193}, {'r': -5.582359, 'l': 120, 't': 466.843587}, {'r': -4.944486, 'l': 120, 't': 466.842928}], maxlen=100)\",\n",
      "  \"ep_success_buffer\": \"deque([], maxlen=100)\",\n",
      "  \"_n_updates\": 5210,\n",
      "  \"observation_space\": \"Box([  0.   0. -30. -30.   0.], [600. 800.  30.  30. 600.], (5,), float32)\",\n",
      "  \"action_space\": \"Box(-1.0, 1.0, (1,), float32)\",\n",
      "  \"n_envs\": 8,\n",
      "  \"n_steps\": 240,\n",
      "  \"gamma\": 0.99,\n",
      "  \"gae_lambda\": 0.95,\n",
      "  \"ent_coef\": 0.0,\n",
      "  \"vf_coef\": 0.5,\n",
      "  \"max_grad_norm\": 0.5,\n",
      "  \"rollout_buffer_class\": \"<class 'stable_baselines3.common.buffers.RolloutBuffer'>\",\n",
      "  \"rollout_buffer_kwargs\": {},\n",
      "  \"batch_size\": 64,\n",
      "  \"n_epochs\": 10,\n",
      "  \"clip_range\": \"FloatSchedule(ConstantSchedule(val=0.2))\",\n",
      "  \"clip_range_vf\": null,\n",
      "  \"normalize_advantage\": true,\n",
      "  \"target_kl\": null,\n",
      "  \"lr_schedule\": \"FloatSchedule(ConstantSchedule(val=0.001))\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 获取模型参数\n",
    "from stable_baselines3 import PPO, TD3, SAC\n",
    "from stable_baselines3.common.save_util import load_from_zip_file\n",
    "import json\n",
    "\n",
    "def extract_model_params(model_path):\n",
    "    \"\"\"提取并显示模型文件中的训练参数\"\"\"\n",
    "    # 从ZIP文件加载数据\n",
    "    data, params, pytorch_variables = load_from_zip_file(model_path)\n",
    "    \n",
    "    # 提取训练参数\n",
    "    training_params = {k: v for k, v in data.items() \n",
    "                      if k not in ['pytorch', 'replay_buffer', '_last_obs']}\n",
    "    \n",
    "    # 打印参数信息\n",
    "    print(f\"模型文件: {model_path}\")\n",
    "    print(\"训练参数:\")\n",
    "    print(json.dumps(training_params, indent=2, default=str))\n",
    "    \n",
    "    # 检查网络架构\n",
    "    if 'policy_kwargs' in training_params and 'net_arch' in training_params['policy_kwargs']:\n",
    "        print(\"\\n网络架构:\")\n",
    "        print(json.dumps(training_params['policy_kwargs']['net_arch'], indent=2))\n",
    "    \n",
    "    return training_params\n",
    "\n",
    "# 使用示例\n",
    "model_params = extract_model_params(\"ball_landing_2.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
