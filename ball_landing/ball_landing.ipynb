{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "#使用PPO算法进行训练自定义游戏环境。游戏环境很简单是二维的，设置为一个小球从天上某个地方自由落体（高度一定，水平位置随机），通过控制左右施加的力让小球落到指定地面范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游戏环境需要实现的方法和返回值\\n__init__(self, render_mode=None)：\\n必须定义action_space和observation_space\\n应该处理render_mode参数\\n\\nreset(self, seed=None, options=None)：\\n必须调用super().reset(seed=seed)\\n必须返回(observation, info)元组\\n\\nstep(self, action)：\\n必须返回(observation, reward, terminated, truncated, info)元组\\n\\nrender(self)：\\n如果支持渲染，必须处理不同的渲染模式\\n对于\"rgb_array\"模式，必须返回RGB数组\\n\\nclose(self)：\\n必须清理所有资源\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"游戏环境需要实现的方法和返回值\n",
    "__init__(self, render_mode=None)：\n",
    "必须定义action_space和observation_space\n",
    "应该处理render_mode参数\n",
    "\n",
    "reset(self, seed=None, options=None)：\n",
    "必须调用super().reset(seed=seed)\n",
    "必须返回(observation, info)元组\n",
    "\n",
    "step(self, action)：\n",
    "必须返回(observation, reward, terminated, truncated, info)元组\n",
    "\n",
    "render(self)：\n",
    "如果支持渲染，必须处理不同的渲染模式\n",
    "对于\"rgb_array\"模式，必须返回RGB数组\n",
    "\n",
    "close(self)：\n",
    "必须清理所有资源\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallLandingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    自定义环境:控制一个从高处落下的球，使其落在指定区域内\n",
    "              @:要求定义的参数内容通过“## 编号.xxxx”标出\n",
    "              @: 参考性的注释通过“#@ 编号.xxx”标出\n",
    "    \n",
    "    状态空间:\n",
    "        - 球的x坐标 (水平位置)\n",
    "        - 球的y坐标 (垂直位置)\n",
    "        - 球的x方向速度\n",
    "        - 球的y方向速度\n",
    "        - 目标区域的中心x坐标\n",
    "    \n",
    "    动作空间:\n",
    "        - 连续值，表示施加在球上的水平力\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 5.metadata是官方建议定义的，包含渲染模式和FPS\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        ## 1.必须调用父类的__init__方法\n",
    "        super(BallLandingEnv, self).__init__()\n",
    "        \n",
    "        # 物理参数\n",
    "        self.gravity = 9.8  # 重力加速度\n",
    "        self.mass = 1.0  # 球的质量\n",
    "        self.time_step = 0.05  # 时间步长\n",
    "        self.max_force = 10.0  # 最大水平力\n",
    "        self.friction = 0.01  # 空气阻力系数\n",
    "        self.vy0=100 #y方向初始速度\n",
    "        \n",
    "        # 环境参数\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 800\n",
    "        self.ball_radius = 15\n",
    "        self.initial_height = 700  # 初始高度\n",
    "        self.target_width = 100  # 目标区域宽度\n",
    "        \n",
    "        ## 2.必须定义动作空间(action_space)\n",
    "        # 动作空间: 连续值，范围为[-1, 1]，表示施加在球上的水平力\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, \n",
    "            high=1.0, \n",
    "            shape=(1,), #@ 1.动作是一个一维向量，包含一个值（标量），表示水平力的大小和方向。例如，action = [0.5] 表示向右施加0.5的力，action = [-0.7] 表示向左施加0.7的力\n",
    "            dtype=np.float32 \n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        观察向量:观察空间定义为 [x, y, vx, vy, target_x]，按照填写顺序，包含 5 个值:\n",
    "        x:小球的水平位置（单位:像素或任意单位）。\n",
    "        y:小球的垂直位置（高度）。\n",
    "        vx:小球的水平速度。\n",
    "        vy:小球的垂直速度。\n",
    "        target_x:目标区域的水平中心位置（例如，目标范围 [-1, 1] 的中心可能是 0）。\n",
    "        \"\"\"\n",
    "        \n",
    "        ## 3.必须定义观察空间(observation_space)\n",
    "        # 观察空间: [x, y, vx, vy, target_x]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -30, -30, 0]), \n",
    "            high=np.array([self.screen_width, self.screen_height, 30, 30, self.screen_width]), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        ## 4.渲染相关的官方要求\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # 初始化状态\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        ## 6.必须定义：调用父类reset方法设置随机种子\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # 初始化球的位置和速度\n",
    "        self.x = self.np_random.uniform(self.ball_radius, self.screen_width - self.ball_radius)\n",
    "        self.y = self.ball_radius\n",
    "        self.vx = 0.0\n",
    "        self.vy = self.vy0\n",
    "        \n",
    "        # 设置目标区域\n",
    "        self.target_x = self.np_random.uniform(\n",
    "            self.target_width / 2, \n",
    "            self.screen_width - self.target_width / 2\n",
    "        )\n",
    "        \n",
    "        # 计算观察\n",
    "        self.state = np.array([self.x, self.y, self.vx, self.vy, self.target_x])\n",
    "        \n",
    "        # 设置步数\n",
    "        self.steps = 0\n",
    "        self.max_steps = 500\n",
    "        \n",
    "        ## 7. 渲染初始化\n",
    "        if self.render_mode == \"human\" and self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.display.set_caption(\"Ball Landing Environment\")\n",
    "        if self.render_mode == \"human\" and self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "            \n",
    "        ## 8. 返回初始观察和信息\n",
    "        return self.state, {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        # 获取动作（水平力）\n",
    "        force_x = float(action[0]) * self.max_force\n",
    "        \n",
    "        # 计算加速度\n",
    "        ax = force_x / self.mass - self.friction * self.vx / self.mass\n",
    "        ay = self.gravity #- self.friction * self.vy / self.mass # Cancel g in y axis\n",
    "        \n",
    "        # 更新速度\n",
    "        self.vx += ax * self.time_step\n",
    "        self.vy += ay * self.time_step\n",
    "        \n",
    "        # 更新位置\n",
    "        self.x += self.vx * self.time_step\n",
    "        self.y += self.vy * self.time_step\n",
    "        \n",
    "        # 检查边界碰撞\n",
    "        if self.x < self.ball_radius:\n",
    "            self.x = self.ball_radius\n",
    "            self.vx = -self.vx * 0.8  # 反弹损失一些能量\n",
    "        elif self.x > self.screen_width - self.ball_radius:\n",
    "            self.x = self.screen_width - self.ball_radius\n",
    "            self.vx = -self.vx * 0.8\n",
    "            \n",
    "        # 检查是否到达地面\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if self.y >= self.screen_height - self.ball_radius:\n",
    "            self.y = self.screen_height - self.ball_radius\n",
    "            terminated = True\n",
    "            \n",
    "            # 计算与目标的距离\n",
    "            distance_to_target = abs(self.x - self.target_x)\n",
    "            \n",
    "            # 根据距离给予奖励\n",
    "            if distance_to_target < self.target_width / 2:\n",
    "                # 在目标区域内\n",
    "                normalized_distance = distance_to_target / (self.target_width / 2)\n",
    "                reward = 10.0 * (1.0 - normalized_distance)  # 越靠近中心奖励越高\n",
    "            else:\n",
    "                # 不在目标区域内\n",
    "                reward = -1.0 - min(distance_to_target / 100, 9.0)  # 距离越远惩罚越大，最低-10\n",
    "        \n",
    "        # 更新状态\n",
    "        self.state = np.array([self.x, self.y, self.vx, self.vy, self.target_x])\n",
    "        \n",
    "        # 增加步数\n",
    "        self.steps += 1\n",
    "        \n",
    "        # 检查是否达到最大步数\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        \n",
    "        ## 9. 渲染，如果渲染模式是human，则渲染环境\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "        ## 10.必须计算自己后续的state, reward, terminated, truncated, info并进行返回。info：额外信息字典，这里是{}\n",
    "        return self.state, reward, terminated, truncated, {}\n",
    "    \n",
    "    \"\"\"\n",
    "    渲染模式检查: 检查self.render_mode并相应地处理。\n",
    "\n",
    "    返回值：\n",
    "    如果渲染模式为\"rgb_array\",返回RGB数组。\n",
    "    如果渲染模式为\"human\",更新显示并控制帧率\n",
    "    \"\"\"\n",
    "    ## 11.渲染模式检查\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            return\n",
    "            \n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "            \n",
    "        # 渲染模式为human时\n",
    "        self._render_frame()\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "    \n",
    "    def _render_frame(self):\n",
    "        ## 12.初始化pygame（如果尚未初始化）\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            \n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "            \n",
    "        # 填充背景\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        # 绘制目标区域\n",
    "        target_left = self.target_x - self.target_width / 2\n",
    "        pygame.draw.rect(\n",
    "            self.screen, \n",
    "            (0, 255, 0), \n",
    "            pygame.Rect(target_left, self.screen_height - 10, self.target_width, 10)\n",
    "        )\n",
    "        \n",
    "        # 绘制球\n",
    "        pygame.draw.circle(\n",
    "            self.screen,\n",
    "            (255, 0, 0),\n",
    "            (int(self.x), int(self.y)),\n",
    "            self.ball_radius\n",
    "        )\n",
    "        \n",
    "        ## 13.定义：如果是rgb_array模式，返回屏幕的像素数组\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), \n",
    "                axes=(1, 0, 2)\n",
    "            )\n",
    "    \n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# 导入环境，在ipynote中不需要\n",
    "#from ball_landing_env import BallLandingEnv\n",
    "\n",
    "# 注册环境\n",
    "register(\n",
    "    id=\"BallLanding-v0\",\n",
    "    entry_point=\"__main__:BallLandingEnv\", #ball_landing_env:BallLandingEnv python文件导入环境，在ipynote中不需要\n",
    ")\n",
    "\n",
    "\n",
    "# 识别模型保存名\n",
    "import os\n",
    "\n",
    "def get_next_model_filename(base_name=\"ppo_ball_landing\"):\n",
    "    \"\"\"\n",
    "    检查现有的模型文件并返回下一个可用的序号文件名\n",
    "    例如：如果已存在 ppo_ball_landing_0.zip，则返回 ppo_ball_landing_1\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        filename = f\"{base_name}_{i}\"\n",
    "        # 检查文件是否存在（注意：SB3保存时会自动添加.zip扩展名）\n",
    "        if not os.path.exists(f\"{filename}.zip\"):\n",
    "            return filename\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO 算法参数详解：\n",
    "\n",
    "Python\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    ")\n",
    "参数详解\n",
    "1. \"MlpPolicy\"\n",
    "含义：指定使用多层感知机(Multilayer Perceptron)作为策略网络的架构。\n",
    "说明：这是一个适用于连续动作空间的默认策略网络。MLP策略使用全连接神经网络来处理状态输入并输出动作。\n",
    "何时调整：如果您的环境有特殊的观察空间（如图像），可能需要使用其他策略如\"CnnPolicy\"。\n",
    "1. env\n",
    "含义：训练环境的实例。\n",
    "说明：这是您之前创建的向量化环境 make_vec_env(\"BallLanding-v0\", n_envs=8)。\n",
    "何时调整：通常不需要调整此参数，它就是您的游戏环境。\n",
    "1. verbose=1\n",
    "含义：控制训练过程中输出信息的详细程度。\n",
    "可选值：\n",
    "0：不输出任何信息\n",
    "1：输出训练的基本信息\n",
    "2：输出更详细的训练信息\n",
    "何时调整：如果您想看到更多/更少的训练日志信息。\n",
    "1. learning_rate=0.0003\n",
    "含义：策略网络和价值网络的学习率。\n",
    "说明：控制每次参数更新的步长大小。0.0003是一个适合多数连续控制任务的值。\n",
    "何时调整：\n",
    "如果训练不稳定或收敛太慢，可以尝试较小的值（如0.0001）\n",
    "如果学习速度太慢，可以尝试较大的值（如0.001）\n",
    "1. n_steps=2048\n",
    "含义：每次更新前收集的环境步数。\n",
    "说明：在执行一次策略更新前，每个环境会执行的步骤数。总样本数 = n_steps × n_envs。\n",
    "何时调整：\n",
    "增大这个值可以使训练更稳定，但会减慢训练速度\n",
    "减小这个值可以加快训练，但可能使训练不稳定\n",
    "1. batch_size=64\n",
    "含义：每次梯度更新使用的小批量样本数。\n",
    "说明：从收集的轨迹中随机抽取的样本数量，用于计算每次更新的梯度。\n",
    "何时调整：\n",
    "增大这个值可以使梯度估计更准确，但会增加计算成本\n",
    "通常建议设为n_steps的因子，以确保所有样本都被使用\n",
    "1. n_epochs=10\n",
    "含义：对每批数据执行策略优化的轮数。\n",
    "说明：对同一批数据重复学习的次数。PPO的一个特点是可以多次使用同一批数据进行多轮更新。\n",
    "何时调整：\n",
    "增大可以提高样本效率，但可能导致过拟合\n",
    "减小可以防止过拟合，但可能降低样本效率\n",
    "1. gamma=0.99\n",
    "含义：折扣因子。\n",
    "说明：用于计算未来奖励的折现值，决定了代理对短期和长期奖励的权衡。\n",
    "取值范围：0到1之间，接近1表示更看重长期奖励。\n",
    "何时调整：\n",
    "对于需要长期规划的任务，设置接近1的值（如0.99或0.999）\n",
    "对于只需要短期反馈的任务，可以设置较小的值（如0.9）\n",
    "1. gae_lambda=0.95\n",
    "含义：广义优势估计(GAE)的λ参数。\n",
    "说明：控制偏差和方差之间的权衡，用于计算优势函数。\n",
    "取值范围：0到1之间。\n",
    "何时调整：\n",
    "较高的值（接近1）会导致更高的方差但偏差更小\n",
    "较低的值会导致更低的方差但偏差更大\n",
    "1.  clip_range=0.2\n",
    "含义：PPO算法中的裁剪参数。\n",
    "说明：限制策略更新的幅度，防止过大的策略变化导致训练不稳定。\n",
    "何时调整：\n",
    "减小这个值可以使训练更稳定，但学习速度会变慢\n",
    "增大这个值可以加快学习，但可能导致训练不稳定\n",
    "1.  tensorboard_log=\"./tensorboard_logs/\"\n",
    "含义：TensorBoard日志的保存路径。\n",
    "说明：指定训练过程中的指标（如奖励、损失等）的保存位置，可以使用TensorBoard可视化这些指标。\n",
    "何时调整：如果您希望将日志保存在不同的位置，或者不想使用TensorBoard（设为None）。\n",
    "如何根据您的环境调整这些参数\n",
    "对于初学者：建议先使用默认参数，观察训练效果。\n",
    "\n",
    "学习率调整：\n",
    "\n",
    "如果训练不稳定（奖励波动大），尝试减小学习率\n",
    "如果学习太慢，尝试增加学习率\n",
    "样本效率调整：\n",
    "\n",
    "增加 n_epochs 和 batch_size 可以提高样本效率\n",
    "但要注意可能导致过拟合和训练不稳定\n",
    "长期vs短期奖励：\n",
    "\n",
    "如果您的任务需要长期规划，保持 gamma 接近1\n",
    "例如在您的球落地环境中，小球需要规划整个下落轨迹，所以较高的gamma值（0.99）是合适的\n",
    "探索与利用平衡：\n",
    "\n",
    "PPO算法会自动调整探索程度，但您可以通过调整 clip_range 间接影响探索行为\n",
    "较大的 clip_range 允许更剧烈的策略变化，可能导致更多探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 创建向量化环境以提高训练效率\n",
    "    # env = make_vec_env(\"BallLanding-v0\", n_envs=8) # python文件使用，ipynb不需要\n",
    "    env = gym.make(\"BallLanding-v0\", render_mode=None)\n",
    "    \n",
    "    # 创建PPO模型\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=0.001,\n",
    "        n_steps=2048,\n",
    "        batch_size=32,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        tensorboard_log=\"./tensorboard_logs/\"\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    total_timesteps = 1_000_000\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "    \n",
    "    # 按序号保存模型\n",
    "    model_filename = get_next_model_filename()\n",
    "    model.save(model_filename)\n",
    "    print(f\"模型已保存为: {model_filename}.zip\")\n",
    "    \n",
    "    # 评估模型\n",
    "    eval_env = gym.make(\"BallLanding-v0\", render_mode=\"human\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # 展示训练后的模型\n",
    "    obs, _ = eval_env.reset()\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            obs, _ = eval_env.reset()\n",
    "            \n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tensorboard_logs/PPO_8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">type: float64</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual \u001b[0m\n",
       "\u001b[33mtype: float64\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method is not within the observation space.</span>\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuchen/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | -1.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 1999     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.41        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1124         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038992735 |\n",
      "|    clip_fraction        | 0.0185       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0936       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0858       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 0.625        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.05       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 961         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002760059 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 0.511       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.13        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 894          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030968022 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.627        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0826       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    std                  | 0.965        |\n",
      "|    value_loss           | 0.58         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.965       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032720028 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    std                  | 0.956        |\n",
      "|    value_loss           | 0.194        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.33        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034781476 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0997       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 0.37         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 837          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047845496 |\n",
      "|    clip_fraction        | 0.0473       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 0.93         |\n",
      "|    value_loss           | 0.0917       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.21        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 828          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073207505 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.016        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 0.0966       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.66        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 823          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060007665 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.661        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0545       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    std                  | 0.93         |\n",
      "|    value_loss           | 0.279        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.52       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004787867 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.62       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004022539 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    std                  | 0.915       |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.37       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 811         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003263435 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.63        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056735687 |\n",
      "|    clip_fraction        | 0.0702       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00308     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    std                  | 0.91         |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.98       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006508837 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 0.0709      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.33        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049953666 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.023        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000902    |\n",
      "|    std                  | 0.932        |\n",
      "|    value_loss           | 0.0579       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.68       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004767446 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.211       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    std                  | 0.925       |\n",
      "|    value_loss           | 0.269       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006622414 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    std                  | 0.918       |\n",
      "|    value_loss           | 0.0896      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.62        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 788          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055639474 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0425       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    std                  | 0.917        |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.58       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 784         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005360119 |\n",
      "|    clip_fraction        | 0.0545      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    std                  | 0.893       |\n",
      "|    value_loss           | 0.0555      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.58       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 782         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009637048 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 0.028       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.63        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 779          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041661044 |\n",
      "|    clip_fraction        | 0.0492       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    std                  | 0.86         |\n",
      "|    value_loss           | 0.106        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.77        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 781          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050375103 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.083        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    std                  | 0.859        |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.87       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006443537 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    std                  | 0.866       |\n",
      "|    value_loss           | 0.0437      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.79       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006351152 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.012       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 0.881       |\n",
      "|    value_loss           | 0.0987      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.62       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006492003 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    std                  | 0.875       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.58        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 775          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067105377 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.000271     |\n",
      "|    std                  | 0.867        |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.38        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 774          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044476185 |\n",
      "|    clip_fraction        | 0.0452       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000847    |\n",
      "|    std                  | 0.867        |\n",
      "|    value_loss           | 0.205        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 774          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047530155 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0436       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    std                  | 0.871        |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006146693 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 0.857       |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.13        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 774          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060066422 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00581     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    std                  | 0.862        |\n",
      "|    value_loss           | 0.0222       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005107592 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    std                  | 0.859       |\n",
      "|    value_loss           | 0.0865      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.12        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 773          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068754796 |\n",
      "|    clip_fraction        | 0.0627       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00549      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    std                  | 0.85         |\n",
      "|    value_loss           | 0.055        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.21        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047424342 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0773       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    std                  | 0.841        |\n",
      "|    value_loss           | 0.102        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -1.33        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060943295 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0795       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    std                  | 0.824        |\n",
      "|    value_loss           | 0.098        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 120        |\n",
      "|    ep_rew_mean          | -0.919     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 92         |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509985 |\n",
      "|    clip_fraction        | 0.04       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0168     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.00269   |\n",
      "|    std                  | 0.827      |\n",
      "|    value_loss           | 0.0712     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.748       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 773          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044281306 |\n",
      "|    clip_fraction        | 0.0504       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.126        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    std                  | 0.813        |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.872      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005749883 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.894       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 774          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058172657 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.161        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00325     |\n",
      "|    std                  | 0.802        |\n",
      "|    value_loss           | 0.19         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.748      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008772881 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00998    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    std                  | 0.797       |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.889      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007056049 |\n",
      "|    clip_fraction        | 0.0553      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00881     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00138    |\n",
      "|    std                  | 0.797       |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.04       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007363255 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    std                  | 0.806       |\n",
      "|    value_loss           | 0.0422      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.19       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 774         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005178244 |\n",
      "|    clip_fraction        | 0.058       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    std                  | 0.803       |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -1.26       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006603266 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00553     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 0.0422      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.754       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057592914 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0641       |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    std                  | 0.805        |\n",
      "|    value_loss           | 0.21         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.946      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006215064 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.000649   |\n",
      "|    std                  | 0.799       |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.546      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009731005 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00758     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    std                  | 0.78        |\n",
      "|    value_loss           | 0.0758      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.833       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067266207 |\n",
      "|    clip_fraction        | 0.0642       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0195       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    std                  | 0.779        |\n",
      "|    value_loss           | 0.188        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.769       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067900224 |\n",
      "|    clip_fraction        | 0.0864       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0294       |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.791        |\n",
      "|    value_loss           | 0.0575       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 772          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 129          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057555046 |\n",
      "|    clip_fraction        | 0.0535       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0373       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 0.791        |\n",
      "|    value_loss           | 0.201        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 120        |\n",
      "|    ep_rew_mean          | -0.368     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 132        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00533053 |\n",
      "|    clip_fraction        | 0.0692     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.117      |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | -0.00265   |\n",
      "|    std                  | 0.794      |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 120          |\n",
      "|    ep_rew_mean          | -0.218       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 770          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034896513 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000229     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    std                  | 0.782        |\n",
      "|    value_loss           | 0.154        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 120         |\n",
      "|    ep_rew_mean          | -0.509      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008299656 |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00058    |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 120        |\n",
      "|    ep_rew_mean          | -0.205     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 108544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01019075 |\n",
      "|    clip_fraction        | 0.0716     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.879      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0684     |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.00223   |\n",
      "|    std                  | 0.756      |\n",
      "|    value_loss           | 0.19       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# 训练模型\u001b[39;00m\n\u001b[1;32m     22\u001b[0m total_timesteps \u001b[39m=\u001b[39m \u001b[39m1_000_000\u001b[39m\n\u001b[0;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps, progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39m# 按序号保存模型\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model_filename \u001b[39m=\u001b[39m get_next_model_filename()\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    312\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    313\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    314\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    315\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    316\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    317\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mep_info_buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_logs(iteration)\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    339\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 275\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    276\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    277\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m~/Fdisk/miniconda3/envs/RL-py39/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import time\n",
    "\n",
    "\n",
    "def test():\n",
    "    # 加载训练好的模型\n",
    "    model = PPO.load(\"ppo_ball_landing\")\n",
    "    \n",
    "    # 创建环境\n",
    "    env = gym.make(\"BallLanding-v0\", render_mode=\"human\")\n",
    "    \n",
    "    # 运行多个回合\n",
    "    for episode in range(10):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # 根据当前观察预测动作\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # 检查是否完成\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # 添加一点延迟，使观察更容易\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 2.41\n",
      "Episode 2: Reward = -1.57\n",
      "Episode 3: Reward = -3.60\n",
      "Episode 4: Reward = 4.91\n",
      "Episode 5: Reward = 6.20\n",
      "Episode 6: Reward = 9.00\n",
      "Episode 7: Reward = -2.42\n",
      "Episode 8: Reward = -1.70\n",
      "Episode 9: Reward = 6.38\n",
      "Episode 10: Reward = -3.11\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n",
    "#查看训练数据用tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 阅读TensorBoard\n",
    "## rollout数据\n",
    "1. rollout/ep_len_mean - 平均回合长度\n",
    "定义：此指标表示每个回合（episode）的平均步数。\n",
    "技术解释：\n",
    "在强化学习中，一个\"回合\"是指从环境重置到终止状态的完整序列\n",
    "这个指标计算了最近N个回合的平均步数\n",
    "在 Stable Baselines3 中，默认是对最近100个回合取平均\n",
    "\n",
    "如何解读曲线：\n",
    "\n",
    "上升趋势：表明代理能够在环境中存活更长时间，通常是积极的信号\n",
    "在\"保持平衡\"类任务中：更长的回合长度直接意味着更好的性能\n",
    "在有固定终止条件的任务中：可能表示代理学会了避免失败条件\n",
    "下降趋势：解读取决于环境类型\n",
    "在\"尽快到达目标\"类任务中：较短的回合长度可能表示代理学会了更有效的策略\n",
    "在大多数其他环境中：可能表示代理性能下降或正在探索新策略\n",
    "稳定值：表示代理的行为模式已经稳定，可能达到了该策略下的最佳表现\n",
    "\n",
    "1. rollout/ep_rew_mean - 平均回合奖励\n",
    "定义：此指标表示每个回合获得的平均总奖励。\n",
    "技术解释：\n",
    "计算最近N个回合（通常是100个）中每个回合获得的总奖励的平均值\n",
    "这是评估代理性能的最直接且最重要的指标\n",
    "在 Stable Baselines3 中实时更新，显示训练过程中的性能变化\n",
    "如何解读曲线：\n",
    "\n",
    "上升趋势：表示代理性能正在改善，这几乎总是积极的信号\n",
    "陡峭上升：快速学习，可能发现了新的有效策略\n",
    "缓慢上升：渐进改善，通常是稳健学习的标志\n",
    "平稳趋势：可能表示学习已经达到饱和或局部最优\n",
    "\n",
    "如果值较高：可能已接近环境的最优性能\n",
    "如果值较低：可能陷入局部最优，需要调整超参数或探索策略\n",
    "下降趋势：通常表示问题\n",
    "\n",
    "暂时下降：可能是探索新策略的过程\n",
    "持续下降：可能是学习率过高或其他超参数问题\n",
    "\n",
    "对于球落地环境：\n",
    "理想的曲线应该从负值或低值开始（初始随机策略）\n",
    "随着训练进行，应该持续上升\n",
    "最终应该达到并稳定在一个正的高值，表示代理已学会将球准确落在目标区域\n",
    "\n",
    "## train数据\n",
    "1. approx_kl - 近似 KL 散度\n",
    "含义：测量更新前后策略分布的差异程度。\n",
    "理想曲线：\n",
    "通常应该在 0.01 到 0.05 之间\n",
    "应相对稳定，略有波动但不应该持续增长\n",
    "偶尔的峰值是正常的，但不应经常出现\n",
    "解读：\n",
    "太低（接近0）：策略几乎没有更新，学习停滞\n",
    "太高（>0.1）：策略变化太剧烈，可能导致训练不稳定\n",
    "稳定在适当范围：表明策略更新适度，学习正常进行\n",
    "\n",
    "2. clip_fraction - 裁剪比例\n",
    "含义：被 PPO 的裁剪机制裁剪的样本比例。\n",
    "理想曲线：\n",
    "训练初期可能较高（0.1-0.3）\n",
    "随着训练进行应逐渐降低并稳定在较低水平（<0.1）\n",
    "解读：\n",
    "持续较高（>0.2）：表明clip_range可能设置得太小，限制了学习\n",
    "几乎为零：可能clip_range太大或学习率太小\n",
    "从高到低再稳定：理想的模式，表明策略逐渐收敛\n",
    "\n",
    "3. clip_range - 裁剪范围\n",
    "含义：PPO 算法中限制策略更新幅度的参数值。\n",
    "理想曲线：\n",
    "如果使用固定值（如0.2），应该是一条水平线\n",
    "如果使用衰减策略，应该是一条平滑下降的曲线\n",
    "解读：\n",
    "这通常是一个设置值而非监控指标\n",
    "确认它是否符合您的预期设置\n",
    "\n",
    "4. entropy_loss - 熵损失\n",
    "含义：衡量策略的不确定性/随机性。\n",
    "理想曲线：\n",
    "训练初期较高，随后逐渐下降\n",
    "不应该太快降到接近零\n",
    "解读：\n",
    "持续较高：策略保持高随机性，可能在过度探索\n",
    "快速降至接近零：策略变得过于确定，可能陷入局部最优\n",
    "缓慢下降：良好的探索-利用权衡，策略逐渐从探索转向利用\n",
    "\n",
    "5. explained_variance - 解释方差\n",
    "含义：价值函数预测与实际回报的匹配程度。\n",
    "理想曲线：\n",
    "从负值或接近0开始，逐渐上升并稳定在接近1的位置\n",
    "解读：\n",
    "接近1：价值函数很好地预测了回报，学习有效\n",
    "接近0：价值函数仅预测平均回报，没有提供额外信息\n",
    "负值：价值函数预测比使用平均值更差，表明严重问题\n",
    "\n",
    "6. loss - 总损失\n",
    "含义：PPO 的总体损失函数。\n",
    "理想曲线：\n",
    "训练初期较高，随后应该逐渐下降并趋于稳定\n",
    "可能有波动，但总体趋势应下降\n",
    "解读：\n",
    "持续下降：学习正常进行\n",
    "停滞不降：可能遇到学习瓶颈\n",
    "剧烈波动：训练不稳定，可能需要调整学习率或批量大小\n",
    "\n",
    "7. policy_gradient_loss - 策略梯度损失\n",
    "含义：代表策略网络更新的主要损失组件。\n",
    "理想曲线：\n",
    "应该是负值（因为 PPO 最大化此值）\n",
    "训练初期可能波动较大，随后应该变得更稳定\n",
    "不应有持续上升趋势（变得更正）\n",
    "解读：\n",
    "持续变得更负：策略正在改进\n",
    "趋于零：策略更新变小，可能接近收敛或学习停滞\n",
    "变为正值或波动剧烈：训练不稳定，需要调整\n",
    "\n",
    "8. std - 动作标准差\n",
    "含义：策略输出的动作分布的标准差。\n",
    "理想曲线：\n",
    "训练初期较高（更多探索）\n",
    "随着训练进行应逐渐减小并稳定（更多利用）\n",
    "不应该太快降到接近零\n",
    "解读：\n",
    "保持较高：策略保持高随机性，持续探索\n",
    "迅速降至接近零：策略变得确定性太快，可能过早收敛\n",
    "缓慢下降并保持合理水平：良好的探索-利用平衡\n",
    "\n",
    "9. value_loss - 价值损失\n",
    "含义：价值函数预测误差的度量。\n",
    "理想曲线：\n",
    "训练初期较高，随后应逐渐下降并稳定\n",
    "可能永远不会降到非常接近零\n",
    "解读：\n",
    "持续下降：价值估计在改进\n",
    "下降后稳定在低水平：价值函数学习良好\n",
    "波动剧烈或上升：价值估计不稳定或环境本身有高方差\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
